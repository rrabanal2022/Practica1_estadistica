---
title: "Práctica 1 estadística"
author: "Ramón Martínez, Rodrigo Rabanal"
format:
  html:
    theme: minty
    css: estilos.css
editor: visual
toc: true
toc-location: left
toc-depth: 2
---

<img src="logo.png" class="logo"/>

# 1. EXPLORACIÓN INICIAL

Antes de empezar, cargamos las librerías que vamos a usar.

```{r message=FALSE warning=FALSE}

library(dplyr)
library(mice)
library(VIM)
library(naniar)
library(car)
library(splines)
library(lmtest)
library(sandwich)
library(ggplot2)
library(MASS)

```

Empezaremos planteando una visualización de los datos, para entender la naturaleza de las variables y ver los valores con los que trabajaremos.

```{r}
datos <- read.csv("athletics_B.csv")

summary(datos)

str(datos)
```

Vamos a realizar un pequeño análisis exploratorio de datos, atendiendo a diferentes aspectos de los mismos.

## 1. Media, mediana y desviación típica

En primer lugar, calcularemos la media, la mediana y la desviación típica del índice de rendimiento de los jugadores.

```{r}
#Calculamos la media del índice de rendimiento de los jugadores

media_performance <- mean(datos$performance_index, na.rm = TRUE)

#Devolvemos el valor calculado

media_performance

#Calculamos la mediana del índice de rendimiento de los jugadores

mediana_performance <- median(datos$performance_index, na.rm = TRUE)

#Devolvemos el valor calculado

mediana_performance

#Calculamos la desviación típica del índice de rendimiento de los jugadores

dtip_performance <- sd(datos$performance_index, na.rm = TRUE)

#Devolvemos el valor calculado

dtip_performance

```

Se aprecia una media y una mediana parecidas, lo que parece indicar que la mayoría de jugadores se encuentran en un estado de forma relativamente similar.

## 2. Histograma de la variable *performance_index*

```{r}
#Vamos a hacer un histograma del índice de rendimiento de los jugadores en 20 intervalos

# Histograma de performance_index con 20 intervalos
hist(datos$performance_index,
     breaks = 20,            
     main = "Histograma de Performance Index",  
     xlab = "Performance Index",                
     ylab = "Frecuencia",                       
     col = "skyblue",                           
     border = "white",
     freq = FALSE) 

lines(density(datos$performance_index),
      col = "red",
      lwd = 2)  

cuartiles <- quantile(datos$performance_index, probs = c(0.25, 0.5, 0.75))

# Añadimos líneas verticales discontinuas en los cuartiles
abline(v = cuartiles, 
       col = "darkgreen", 
       lty = 2,      # tipo de línea discontinua
       lwd = 2)      # grosor


```

En el histograma, podemos apreciar que la variable *performance_index* sigue una distribución normal.

La línea roja marca la distribución, mientras que las líneas verdes discontinuas marcan los valores de los cuartiles; entre los que se encuentra el 50% de la muestra para esta variable.

Esto complementa a lo que hemos deducido previamente a partir de los datos de la media y la mediana.

## 3. Distribución de frecuencias relativas

```{r}
#Vamos a calcular la distribución de las frecuencias relativas de los jugadores por posición

frecuencia <- table(datos$position)

frecuencia_relativa <- prop.table(frecuencia)

# Mostrar la tabla con frecuencias absolutas y relativas (en porcentaje)
tabla_frecuencias <- data.frame(
  Posición = names(frecuencia),
  Frecuencia = as.vector(frecuencia),
  Frecuencia_Relativa = round(as.vector(frecuencia_relativa) * 100, 2)
)

# Mostrar resultados
tabla_frecuencias

```

Podemos observar que la frecuencia relativa de porteros es mucho menor que la del resto de posiciones. Esto se debe a que en un equipo juega un solo portero para los 3 o 4 defensas, 3 o 4 mediocentros y 2 o 3 delanteros. Por tanto, es lógico obtener ese valor. Caso análogo, pero en menor medida para los delanteros, que suponen un número inferior en la plantilla que los mediocentros y los defensas.

## 4. Diagrama de cajas de *performace_index* según el equipo

```{r}

# Diagrama de cajas de performance_index según team
boxplot(performance_index ~ team, 
        data = datos,
        main = "Diagrama de Cajas de Performance Index por Equipo",
        xlab = "Equipo",
        ylab = "Performance Index",
        col = c("lightblue", "lightgreen", "lightpink", "khaki"),
        border = "darkgray",
        notch = TRUE)

```

En este *boxplot* podemos ver que en el equipo B hay una mayor variabilidad de resultados, pues los cuartiles son más grandes que los de los equipos A y B (que tienen niveles más similares de rendimiento). Sin embargo, la mediana del equipo B es claramente mayor que la del resto de equipos.

Esto puede indicar un mayor nivel de algunos jugadores del equipo B con respecto de los jugadores del resto de equipos, pero también entre los jugadores del mismo equipo (mucha diferencia entre jugadores "estrellas" y el resto).

Por último, en el equipo C tienen todos los jugadores un nivel muy similar; lo que podría provocar que el rendimiento medio sea mayor.

## 5. Gráfico de dispersión entre *training_hours* y *performance_index*

```{r}
#Construimos un gráfico de dispersión entre training_hours y performance_index
plot(datos$training_hours, datos$performance_index,
     main = "Relación entre horas de entrenamiento y rendimiento",
     xlab = "Horas de entrenamiento (training_hours)",
     ylab = "Índice de rendimiento (performance_index)",
     pch = 19,               # tipo de punto sólido
     col = "steelblue",      # color de los puntos
     cex = 1.2)              # tamaño de los puntos

# Agregar una línea de tendencia (regresión lineal)
abline(lm(performance_index ~ training_hours, data = datos), 
       col = "red", lwd = 2)
```

Se aprecia linealidad en el modelo. Hay una clara relación lineal entre las horas de entrenamiento y el índice de rendimiento, apreciando un mayor rendimiento conforme aumentan las horas de entrenamiento.

## 6. Matriz de correlaciones

```{r}
#Calculamos la matriz de correlaciones entre variables numéricas
datos_numericos <- datos[, sapply(datos, is.numeric)]

# Calcular la matriz de correlaciones
matriz_cor <- cor(datos_numericos, use = "complete.obs")

# Mostrar la matriz de correlaciones
matriz_cor

#Vamos a visualizarla
library(corrplot)

corrplot(matriz_cor, method = "color", type = "upper",
         tl.col = "black", tl.cex = 0.8, 
         addCoef.col = "black", number.cex = 0.4)

```

Aquí podemos ver la matriz de correlaciones. Vemos que las variables que presentan más correlación son los pases realizados con éxito y los pases intentados. Otras variables que presentan cierta correlación son el índice de rendimiento con la edad y las horas de entrenamiento.

## 7. Gráfico de dispersión entre fatiga e índice de rendimiento

```{r}

#Representamos un gráfico de dispersión entre training_hours y performance_index
library(ggplot2)

ggplot(datos, aes(x = fatigue, y = performance_index)) +
  geom_point(color = "steelblue", size = 2) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Relación entre fatiga e índice de rendimiento",
       x = "Fatiga",
       y = "Índice de rendimiento") +
  theme_minimal()

```

Parece que hay una relación lineal entre ambas variables. En particular, podemos observar que parece haber una relación que indica una menor índice de rendimiento conforme la fatiga incrementa.

## 8. Boxplot del índice de rendimiento por posición

```{r}
#Realizamos un boxplot de performance_index por posicion

boxplot(performance_index ~ position,
        data = datos,
        main = "Diagrama de Cajas de Performance Index por Posición",
        xlab = "Posición",
        ylab = "Performance Index",
        col = c("lightblue", "lightgreen", "lightpink", "khaki"),
        border = "gray40",
        notch = TRUE)


```

Se aprecian una serie de diferencias entre puestos, que procederemos a desarrollar. En primer lugar, no hay grandes diferencias en lo referente a la media del índice de rendimiento en función de la posición. Por otro lado, vemos que el tercer cuartil de los defensas es más pequeño que el resto, dando a entender que se encuentran más agrupados en esas zonas de rendimiento; destacando menos que en otras posiciones. Además, hay tres outliers con valores muy bajos de rendimiento. Finalmente, el último valor remarcable es el del portero, donde vemos que el "bigote" es de longitud mucho menor que el del resto de posiciones.

## 9. Valores atípicos en *agility_time*

```{r}
# Vamos a identificar los valores atípicos en agility_time 

# Calculamos los cuartiles y el rango intercuartílico
Q1 <- quantile(datos$agility_time, 0.25, na.rm = TRUE)
Q3 <- quantile(datos$agility_time, 0.75, na.rm = TRUE)
IQR_value <- IQR(datos$agility_time, na.rm = TRUE)

# Límites inferior y superior para detectar atípicos
lim_inf <- Q1 - 1.5 * IQR_value
lim_sup <- Q3 + 1.5 * IQR_value

# Identificamos los valores atípicos
outliers <- datos$agility_time[datos$agility_time < lim_inf | datos$agility_time > lim_sup]

# Mostramos resultados
outliers

```

Vemos que por la regla del rango intercuartílico (IQR) hay 5 valores atípicos, que se corresponden con NA. Es decir, valores no recogidos de esta variable para ciertos jugadores. Vamos a hacer un boxplot para intentar ver más posibles valores atípicos, donde estos hayan sido recogidos.

```{r}
boxplot(datos$agility_time,
        main = "Detección de valores atípicos en Agility Time",
        ylab = "Agility Time",
        col = "lightblue",
        border = "gray40")

```

La conclusión del boxplot es la misma. No hay valores atípicos en la variable agility_time, más allá de los valores no recogidos.

## 10. Proporción de jugadores zurdos en la base de datos

```{r}

# Calculamos la proporción de jugadores zurdos
proporcion_zurdos <- mean(datos$left_footed == 1, na.rm = TRUE)

# Mostramos el resultado en porcentaje
proporcion_zurdos * 100


```

Como podemos ver, el 29,17% de los jugadores de la base de datos son zurdos.

# 2. MANEJO DE VALORES FALTANTES

## 1. Identificación de variables con valores faltantes

Primero, vemos qué columnas tienen valores NA

```{r}

# Ver columnas que contienen al menos un NA
colnames(datos)[colSums(is.na(datos)) > 0]


```

Las variables *training_hours*, *sleep_hours*, *fatigue*, *strength_1RM* y *agility_time* tienen valores faltantes.

Ahora, veremos cuántos valores faltan en cada una

```{r}
# Cantidad de NA solo en columnas donde hay al menos uno
na_counts <- colSums(is.na(datos))
na_counts[na_counts > 0]


```

Así, vemos que hay 5 valores faltantes en cada una de estas variables.

## 2. Calculamos la proporción de valores faltantes sobre el dataset completo

```{r}

# Porcentaje total de NA
mean(is.na(datos)) * 100


```

Es decir, el 0.25% de los valores del dataset son valores faltantes.

## 3. Resumen de combina las variables con missing y su porcentaje de valores ausentes

```{r}

# Calcular cantidad y porcentaje de NA por variable
na_table <- data.frame(
  variable = colnames(datos),
  na_count = colSums(is.na(datos)),
  na_percent = (1- colMeans(is.na(datos)) * 100)
)

# Filtrar solo variables con al menos un NA
na_table <- na_table[na_table$na_count > 0, ]

# Mostrar tabla
na_table


```

## 4. Imputación por la media en *training_hours*

```{r}

# Calculamos la media de training_hours ignorando NA
media_training <- mean(datos$training_hours, na.rm = TRUE)

# Imputamos los NA con la media
training_hours_imputada <- datos$training_hours
training_hours_imputada[is.na(training_hours_imputada)] <- media_training

# Calculamos de nuevo la media después de imputar
nueva_media_training <- mean(training_hours_imputada)

# Comprobamos que el valor es el mismo
media_training
nueva_media_training


```

Con esto, hemos sustituido los valores faltantes de la variable *training_hours* con la media de los valores de esta variable.

## 5. Histogramas de *sleep_hours* antes y después de la imputación por la media

En primer lugar, trabajaremos con la variable sin imputar (con valores faltantes).

```{r}

# Guardamos una copia de la variable original
sleep_original <- datos$sleep_hours

# Calculamos la media sin NA
media_sleep <- mean(datos$sleep_hours, na.rm = TRUE)

```

A continuación, trabajaremos con la versión imputada.

```{r}

# Creamos la versión imputada
sleep_imputada <- datos$sleep_hours
sleep_imputada[is.na(sleep_imputada)] <- media_sleep

```

Ahora, podemos desarrollar los histogramas.

```{r}

# Histogramas antes y después
par(mfrow = c(1, 2))  # Mostrar dos gráficos juntos

hist(sleep_original,
     main = "Sleep Hours - Antes imputación",
     xlab = "Horas de sueño")

hist(sleep_imputada,
     main = "Sleep Hours - Después imputación con media",
     xlab = "Horas de sueño")


```

Como podemos ver, no se aprecian diferencias aparentes. Esto se debe al hecho de que, al imputar los valores faltantes por la media, no modificamos la estructura presente de la variable. Así, el único cambio aparente es un aumento de la frecuencia para los valores cercanos a la mediana (asumimos que son los imputados). Sin embargo, como afirmamos, la distribución es la misma.

## 6. Imputación por la mediana en *fatigue* y comparación con imputación con la media.

En primer lugar, imputamos los valores por la mediana en la variable *fatigue*.

```{r}
# Calculamos la mediana ignorando NA
mediana_fatigue <- median(datos$fatigue, na.rm = TRUE)

# Crear una nueva variable con la imputación
fatigue_imputada <- datos$fatigue
fatigue_imputada[is.na(fatigue_imputada)] <- mediana_fatigue

# Comprobamos que ya no hay NA
sum(is.na(fatigue_imputada))


```

A continuación, hacemos la imputación por la media.

```{r}

# Calcular la media ignorando NA
media_fatigue <- mean(datos$fatigue, na.rm = TRUE)

# Crear nueva variable imputada por la media
fatigue_imputada_media <- datos$fatigue
fatigue_imputada_media[is.na(fatigue_imputada_media)] <- media_fatigue

# Comprobar la imputación
sum(is.na(fatigue_imputada_media))  


```

Ahora, porcedemos a comparar estas nuevas variables creadas.

Para ello, empezaremos comparando estadísticas básicas mediante una tabla

```{r}

# Media original (ignorando NA)
media_original <- mean(datos$fatigue, na.rm = TRUE)

# Media tras imputación por mediana
media_median <- mean(fatigue_imputada, na.rm = TRUE)

# Media tras imputación por media
media_mean <- mean(fatigue_imputada_media, na.rm = TRUE)

# Mediana original
mediana_original <- median(datos$fatigue, na.rm = TRUE)

# Mediana tras imputación por mediana
mediana_median <- median(fatigue_imputada, na.rm = TRUE)

# Mediana tras imputación por media
mediana_mean <- median(fatigue_imputada_media, na.rm = TRUE)

data.frame(
  Version = c("Original", "Imputación Mediana", "Imputación Media"),
  Media = c(media_original, media_median, media_mean),
  Mediana = c(mediana_original, mediana_median, mediana_mean)
)


```

De esta manera, vemos que la imputación por la mediana modifica ligeramente la media de la variable; mientras que el valor de la mediana permanece intacto en cualquier caso.

Vamos a ver si este cambio es significativo. Para ello, haremos un *t-test*, cuya hipótesis nula será que el cambio no es significativo.

```{r}

# Valores originales sin NA
original <- datos$fatigue[!is.na(datos$fatigue)]

# Valores imputados por mediana
imputada_mediana <- fatigue_imputada

# Posiciones donde había NA
na_pos <- which(is.na(datos$fatigue))

# Comparar solo los valores imputados (NA reemplazados por mediana) con los originales
imputados_vs_original <- cbind(original = datos$fatigue, imputada = imputada_mediana)

# Para test t, usamos los NA reemplazados
# Aquí hacemos un t-test aproximado usando todos los valores, aunque no es estrictamente válido
t.test(original, imputada_mediana[1:length(original)], paired = TRUE)


```

El p-valor resulta muy elevado, por lo que no podemos rechazar la hipótesis nula. De esta manera, concluimos que la diferencia no es estadísticamente significativa.

A continuación, haremos una comparación de histogramas, para ver si a nivel visual hay una diferencia apreciable.

```{r}

par(mfrow = c(1, 3))  # Mostrar tres gráficos juntos

hist(datos$fatigue, main = "Original", xlab = "Fatigue", col = "lightblue")
hist(fatigue_imputada, main = "Imputación Mediana", xlab = "Fatigue", col = "lightgreen")
hist(fatigue_imputada_media, main = "Imputación Media", xlab = "Fatigue", col = "lightpink")



```

Como se puede ver, a nivel visual apenas hay diferencia. Simplemente, en ambas imputaciones,la barra que abarca los valores de la mediana incrementa (ya que la media es próxima a la mediana), pero mantienen la estructura.

Así, concluimos que apenas hay diferencias entre ambas imputaciones.

## 7. Imputación condicional en *strength_1RM*

```{r}

# Creamos una nueva variable imputada sin modificar la original
strength_1RM_imputada <- datos$strength_1RM

# Calcular media por posición y reemplazamos los NA
strength_1RM_imputada <- datos %>%
  group_by(position) %>%
  mutate(strength_1RM_imputada = ifelse(
    is.na(strength_1RM),
    mean(strength_1RM, na.rm = TRUE),
    strength_1RM
  )) %>%
  ungroup() %>%
  pull(strength_1RM_imputada)

# Comprobar que los NA fueron imputados
sum(is.na(strength_1RM_imputada))  # Debe ser 0

```

Una vez creada la variable, verificamos que cumple los objetivos y actualizaremos la variable *strength_1RM* del conjunto de datos.

```{r}

# Media por posición antes de imputar
media_original <- datos %>%
  group_by(position) %>%
  summarise(media_strength = mean(strength_1RM, na.rm = TRUE))


# Añadimos la variable al dataset
datos$strength_1RM_imputada <- strength_1RM_imputada  

# Media por posición después de imputar
media_imputada <- datos %>%
  group_by(position) %>%
  summarise(media_strength = mean(strength_1RM_imputada, na.rm = TRUE))

# Eliminamos la variable del dataset
datos$strength_1RM_imputada <- NULL


# Mostrar resultados
media_original
media_imputada

```

Vemos que los valores coinciden, por lo que la imputación ha sido realizada de forma correcta.

## 8. Comentario acerca de los valores faltantes

En primer lugar, para ello, vamos a ver el patrón de los NA. Es decir, vamos a ver dónde aparecen los NA, si hay combinaciones de NA entre variables o si aparecen de forma aleatoria.

```{r}

md.pattern(datos)

aggr(datos, prop = TRUE, numbers = TRUE)

```

Como podemos apreciar, no hay NA agrupados en la misma observación. Por tanto, podemos intuir que los valores faltantes son fruto del azar (MCAR). Por tanto, nos ayudaremos de un test estadístico que nos diga si lo son (el test mcar).

```{r}
# Visualizar proporción de NA por variable
vis_miss(datos)

# Ver patrón de NA entre variables
gg_miss_upset(datos)

mcar_test(datos)


```

Como el p-valor es mayor de 0.05, no podemos rechazar que sean MCAR.

Con esto y la información anterior, podemos concluir que los valores faltantes que tenemos son MCAR, al provenir completamente del azar.

## 9. Modelo lineal simple entre *performance_index* y *training_hours*

```{r}

# Ajustar modelo lineal simple usando solo casos completos
modelo <- lm(performance_index ~ training_hours, data = datos, na.action = na.omit)

# Resumen del modelo
summary(modelo)


```

Como tenemos un modelo con 514 grados de libertad, es fácil deducir que se han perdido 5 observaciones por NA.

A continuación, vamos a ver qué filas se han perdido exactamente.

```{r}

# Filas completas (sin NA) en performance_index y training_hours
casos_completos <- complete.cases(datos[, c("performance_index", "training_hours")])

# Filas perdidas (con NA)
filas_perdidas <- which(!casos_completos)

# Mostramos los índices de las filas perdidas
filas_perdidas


```

## 10. Modelo lineal con valores imputados

```{r}

# Ajustar modelo lineal usando la variable imputada
modelo_imputado <- lm(datos$performance_index ~ training_hours_imputada)

# Resumen del modelo
summary(modelo_imputado)


```

Vemos la diferencia entre los coeficientes de los modelos

```{r}

# Coeficientes del modelo original
coef(modelo)

# Coeficientes del modelo imputado
coef(modelo_imputado)


```

Los coeficientes han cambiado ligeramente, al cambiar el número de observaciones, con valores añadidos manualmente. Sin embargo, la diferencia es mínima, al haber muy pocos valores faltantes, por lo que los cambios no influyen prácticamente en el modelo; ya que los coeficientes cambian mínimamente.

# 3. REGRESIÓN LINEAL SIMPLE Y MÚLTIPLE

## 1. Modelo de regresión lineal simple entre *performance_index* y *training_hours*

Emplearemos las variables imputadas comentadas previamente.

```{r}

# Resumen del modelo
summary(modelo_imputado)


```

La pendiente (con valor 1.7112) indica que, por cada hora de entrenamiento, el valor del índice de rendimiento aumenta 1.7112.

## 2. Gráfico de dispersión del modelo

```{r}

# Scatterplot
plot(datos$training_hours, datos$performance_index,
     main = "Performance Index vs Training Hours",
     xlab = "Training Hours",
     ylab = "Performance Index",
     pch = 19, col = "blue")

# Añadir recta ajustada del modelo imputado
abline(lm(datos$performance_index ~ training_hours_imputada), col = "red", lwd = 2)

# Leyenda
legend("topleft", legend = c("Datos", "Recta modelo imputado"),
       col = c("blue", "red"), pch = c(19, NA), lty = c(NA, 1), lwd = c(NA, 2))


```

## 3. Cálculo y análisis del R\^2 del modelo

En primer lugar, calcularemos el R\^2 del modelo.

```{r}

# R^2 del modelo imputado
summary(modelo_imputado)$r.squared

```

El R\^2 vale 0.3279338. Esto quiere decir que el modelo explica un 32.79% de la variabilidad del índice de rendimiento.

## 4. Modelo de regresión lineal múltiple

Vamos a ajustar un modelo de regresión lineal múltiple con la variable *performance_index* explicada a partir de *training_hours*, *match_intensity* y *strength_1RM*.

```{r}

# Ajustar modelo lineal múltiple
modelo_multiple <- lm(performance_index ~ training_hours + match_intensity + strength_1RM, data = datos, na.action = na.omit)

# Resumen del modelo
summary(modelo_multiple)


```

La variable que más efecto tiene sobre el rendimiento es *training_hours*, cuyo coeficiente asociado vale 1.7253 (mucho mayor que el valor absoluto del resto).

## 5. Comparación con el modelo simple

```{r}

# Coeficiente del modelo simple
coef_simple <- coef(modelo)["training_hours"]
coef_simple

# Coeficiente del modelo múltiple
coef_multiple <- coef(modelo_multiple)["training_hours"]
coef_multiple

# Intercepto del modelo simple
intercepto_simple <- coef(modelo)["(Intercept)"]
intercepto_simple

# Intercepto del modelo múltiple
intercepto_multiple <- coef(modelo_multiple)["(Intercept)"]
intercepto_multiple

```

Vemos, de esta manera, que el coeficiente de la variable *training_hours* aumenta en el modelo múltiple y el intercepto disminuye su valor.

Esto se debe a que los coeficientes asociados a las otras variables le restan valor a las horas de entrenamientos, ajustándose así a la realidad que permite explicar el modelo. Por otro lado, tiene sentido que el intercepto sea menor, ya que la pendiente parece más pronunciada por este incremento del coeficiente asociado a la variable *training_hours*.

## 6. Adición de la variable *fatigue*

```{r}

# Ajustar modelo múltiple con fatigue
modelo_multiple2 <- lm(performance_index ~ training_hours + match_intensity + strength_1RM + fatigue, data = datos, na.action = na.omit)

# Resumen del modelo
summary(modelo_multiple2)


```

El R\^2 ajustado ha mejorado tras añadir esta nueva variable al modelo. Esto significa que es útil, ya que el hecho de haberla añadido permite explicar cierta variabilidad del índice de rendimiento.

## 7. Efecto de la variable *position*

Si metemos la variable categórica *position* en un modelo de regresión lineal múltiple con predictores cuantitativos, la variable categórica se convierte en valores Dummy de la siguiente manera:

```{r}

# Limpiar la variable
datos$position <- factor(trimws(tolower(datos$position)))

# Crear dummies
dummy_matrix <- model.matrix(~ position, data = datos)
dummy_df <- as.data.frame(dummy_matrix[, -1])

# Tabla de correspondencia: un nivel por fila
tabla_correspondencia <- unique(cbind(position = datos$position, dummy_df))
tabla_correspondencia




```

En este caso, se habrían estimado estos coeficientes Dummy para las posiciones.

El que tiene todo 0 es la referencia (en este caso, los defensas). El resto de coeficientes Dummy se comparan con este nivel.

El modelo devolverá un valor entre la diferencia de posiciones, que permite determinar la diferencia de rendimiento entre jugadores de las mismas.

## 8. Incluimos *sleep_hours* como predictor adicional

```{r}

modelo_multiple3 <- lm(performance_index ~ training_hours + match_intensity +
                         strength_1RM + fatigue + sleep_hours,
                       data = datos)
summary(modelo_multiple3)

```

Con esto, podemos ver que la fatiga influye negativamente en el rendimiento de los jugadores. De esta manera, el descanso será necesario para obtener un buen rendimiento.

## 9. Diagrama de residuos del modelo múltiple

```{r}

plot(modelo_multiple3, which = 1)


```

Los puntos aparecen dispersos aparentemente al azar en torno a la línea central. Este es un indicio de linealidad, por lo que no hay ningún motivo aparente como para descartarla. Es verdad que hay un ligero indicio de curva, pero no lo suficiente como para tomar la conclusión de descartar la linealidad.

## 10. Modelo lineal simple vs múltiple

Para este dataset, conviene más utilizar un modelo lineal simple cuando se quiere ver de forma "pura" el efecto de una sola variable para el explicar el rendimiento o cualquier otra variable. Sin embargo, si queremos explicar el por qué de una variable o de su distribución, esto no se va a deber solo a un predictor, sino que habrá que tomar varios que lo expliquen.

De esta manera, nos conviene usar un modelo simple para ver cómo influye una variable en las demás, pero nos conviene usar un modelo múltiple para entender el comportamiento real de una variable a partir de las demás (y que se recoja mejor toda su variabilidad, atendiendo a todos sus posibles condicionantes que la alteren).

# 4. NO LINEALIDADES Y PIECEWISE

## 1. Modelo no lineal

```{r}

# Creamos el modelo lineal y el cuadrático

modelo_lineal_training <- lm(performance_index ~ training_hours, data = datos)
summary(modelo_lineal_training)

modelo_cuadratico_training <- lm(performance_index ~ training_hours + I(training_hours^2), data = datos)
summary(modelo_cuadratico_training)


```

A continuación, compararemos los modelos con ANOVA, con R\^2 ajustado y con AIC

```{r}

anova(modelo_lineal_training, modelo_cuadratico_training)


```

Vemos que el p-valor del test es bastante menor que 0.05, lo cual indica que el modelo cuadrático es mejor, ya que explica mejor la varianza en el modelo.

```{r}

summary(modelo_lineal_training)$adj.r.squared
summary(modelo_cuadratico_training)$adj.r.squared


```

Vemos que el R\^2 del modelo cuadrático es mayor que el del modelo lineal, por lo que hay más porcentaje de variabilidad explicada de la variable *performance_index* en el modelo cuadrátrico.

```{r}

AIC(modelo_lineal_training, modelo_cuadratico_training)

```

Vemos que en el modelo cuadrático hay un valor menor de AIC. Esto implica que será un mejor modelo, logrando un mejor ajuste que el modelo lineal sin añadir excesiva complejidad.

Con todo esto, podemos concluir que el modelo cuadrático es notablemente mejor que el modelo lineal.

## 2. Relación entre *performance_index* y *age*

En primer lugar, haremos un gráfico de dispersión para ver la relación

```{r}

plot(datos$age, datos$performance_index,
     xlab = "Edad",
     ylab = "Performance Index",
     main = "Relación entre Performance y Edad",
     pch = 19, col = "blue")


```

Esto nos permite intuir que el índice de rendimiento parece ser mayor conforme la edad aumenta. Sin embargo, para verificarlo y detectar formas cuadráticas, haremos un modelo cuadrático.

```{r}

modelo_age <- lm(performance_index ~ age + I(age^2), data = datos)
summary(modelo_age)


```

Efectivamente, tal y como intuíamos, el índice de rendimiento incrementa conforme la edad aumenta.

Además, vemos que *age\^2* es negativo en el modelo. Con esto, sabemos que obtendremos un pico en el modelo. Para verlo mejor, añadiremos al gráfico anterior la curva del modelo cuadrático.

```{r}

plot(datos$age, datos$performance_index,
     xlab = "Edad",
     ylab = "Performance Index",
     main = "Relación entre Performance y Edad",
     pch = 19, col = "blue")

age_seq <- seq(min(datos$age), max(datos$age), length.out = 100)
pred <- predict(modelo_age, newdata = data.frame(age = age_seq))

lines(age_seq, pred, col = "red", lwd = 2)


```

Así, la relación es cuadrática, aunque parece tener una tendencia suave; no muy pronunciada. Vamos a calcular el valor del pico y lo añadiremos a la gráfica anterior.

```{r}

beta <- coef(modelo_age)
age_pico <- -beta["age"] / (2 * beta["I(age^2)"])
age_pico
performance_pico <- predict(modelo_age, newdata = data.frame(age = age_pico))


```

Así, el pico de rendimiento se da a los 31.80029 años. A partir de entonces, el rendimiento empieza a bajar. A continuación aparece el gráfico completo con el pico marcado.

```{r}

# Gráfico de dispersión
plot(datos$age, datos$performance_index,
     xlab = "Edad",
     ylab = "Performance Index",
     main = "Rendimiento vs Edad con Curva Cuadrática",
     pch = 19, col = "blue")

# Agregar la curva cuadrática ajustada
lines(age_seq, pred, col = "red", lwd = 2)

# Marcar el pico en el gráfico
points(age_pico, performance_pico, col = "black", pch = 19, cex = 1.5)
text(age_pico, performance_pico, 
     labels = paste0(round(age_pico, 1), " años"), 
     pos = 3, col = "black")


```

## 3. Variable dicotómica para *sleep_hours*

```{r}

# Creamos sleep_group: 0 = <6h, 1 = ≥6h
datos$sleep_group <- ifelse(datos$sleep_hours < 6, 0, 1)

# Convertir a factor para que R lo trate como categoría
datos$sleep_group <- factor(datos$sleep_group, labels = c("<6h", "≥6h"))


```

Una vez creada la variable, la ajustaremos como predictora del modelo

```{r}

modelo_sueño <- lm(performance_index ~ sleep_group, data = datos)
summary(modelo_sueño)

```

Analizando la salida, vemos que los jugadores que han dormido más de 6 horas incrementan su rendimiento. El intercepto indica la media de rendimiento de jugadores con menos de 6 horas de sueño, mientras que el otro coeficiente (2.436) indica la diferencia entre los jugadores que han dormido más de 6 horas con respecto de aquellos que no. De esta manera, un mayor descanso deriva en un mejor rendimiento.

Por último, vamos a plantear un boxplot para visualizar mejor esta diferencia.

```{r}

boxplot(performance_index ~ sleep_group, data = datos,
        xlab = "Horas de sueño",
        ylab = "Performance Index",
        main = "Rendimiento según horas de sueño",
        col = c("lightblue", "lightgreen"))


```

## 4. *performance_index* frente a *fatigue* empleando un LOESS

```{r}

library(ggplot2)

ggplot(datos, aes(x = fatigue, y = performance_index)) +
  geom_point(color = "blue") +
  geom_smooth(method = "loess", color = "red", se = TRUE) +
  labs(title = "Rendimiento vs Fatigue con LOESS",
       x = "Fatigue",
       y = "Performance Index") +
  theme_minimal()


```

Podemos ver en la gráfica que la relación planteada es no lineal, con dos pequeños "picos" y un "valle" entre medias, y una posterior caída.

## 5. Spline sobre *training_hours* y comparación con método cuadrático

```{r}


# Ajustamos un spline cúbico con 4 knots por defecto
modelo_spline <- lm(performance_index ~ bs(training_hours, df = 4), data = datos)
summary(modelo_spline)


```

A continuación, graficaremos ambos modelos para ver la diferencia

```{r}

library(ggplot2)

ggplot(datos, aes(x = training_hours, y = performance_index)) +
  geom_point(color = "blue", alpha = 0.6) +  # puntos de datos
  stat_smooth(aes(color = "Cuadrático"), method = "lm", formula = y ~ poly(x, 2), se = FALSE, linetype = "dashed", size = 1) +
  stat_smooth(aes(color = "Spline"), method = "lm", formula = y ~ bs(x, 4), se = FALSE, size = 1) +
  scale_color_manual(name = "Modelo", values = c("Cuadrático" = "red", "Spline" = "darkgreen")) +
  labs(title = "Comparación: Modelo cuadrático vs Spline",
       x = "Training Hours", y = "Performance Index") +
  theme_minimal()


```

Vemos que el spline recoge mejor la no linealidad de la relación, ajustándose mejor a los valores reales.

Por otro lado, vamos a comparar formalmente ambos ajustes.

```{r}

# AIC
AIC(modelo_cuadratico_training, modelo_spline)

# R² ajustado
summary(modelo_cuadratico_training)$adj.r.squared
summary(modelo_spline)$adj.r.squared


```

Vemos que el AIC del modelo cuadrático es algo mayor, por lo que se ajusta un poco mejor a la relación. Sin embargo, el R\^2 del modelo spline es mayor, explicando más variabilidad.

La conclusión que podemos sacar es que ambos modelos son muy similares, explicando una variabilidad parecida. De esta manera, no parece que uno sea mucho mejor que el otro.

## 6. Piecewise en travel_km_week

En primer lugar, vamos a crear la variable dicotómica *travel_group*.

```{r}

datos$travel_group <- ifelse(datos$travel_km_week <= 1500, "≤1500", ">1500")

datos$travel_group <- factor(datos$travel_group, levels = c("≤1500", ">1500"))


```

A continuación, ajustaremos el modelo lineal con esta variable.

```{r}

modelo_piecewise <- lm(performance_index ~ travel_group, data = datos)
summary(modelo_piecewise)

```

Con la salida del modelo, vemos que el índice de rendimiento disminuye para aquellos jugadores que han viajado más de 1500 km a la semana. Esto se puede comprobar ya que el coeficiente asociado a esta variable es negativo (-5.9998). El valor de este coeficiente será la diferencia con los jugadores pertenecientes al otro grupo.

Finalmente, dispondremos un boxplot para ver la diferencia de manera visual.

```{r}

boxplot(performance_index ~ travel_group, data = datos,
        xlab = "Distancia semanal de viaje",
        ylab = "Performance Index",
        main = "Rendimiento según travel_km_week",
        col = c("lightblue", "lightgreen"))


```

## 7. Evaluación del AIC entre modelos lineal, cuadrático y piecewise para *training_hours*

Tenemos el modelo lineal y el cuadrático creados en anteriores apartados. Nos falta, por tanto, crear el modelo piecewise.

Para ello, en primer lugar crearemos la variable dicotómica *training_group*.

```{r}

datos$training_group <- ifelse(datos$training_hours <= 20, "≤20", ">20")
datos$training_group <- factor(datos$training_group, levels = c("≤20", ">20"))


```

Con esta, creamos el modelo piecewise

```{r}

modelo_piecewise_training <- lm(performance_index ~ training_group, data = datos)

```

Ahora, podremos comparar el AIC de estos modelos

```{r}

AIC(modelo_lineal_training, modelo_cuadratico_training, modelo_piecewise_training)

```

El modelo piecewise es el de mayor AIC, mientras que el cuadrático es el de menor AIC. De esta manera, el mejor modelo será el cuadrático, ya que es el que mejor combina ajuste y simplicidad.

## 8. Representación gráfica de las predicciones del modelo piecewise frente a las observaciones reales

En primer lugar, hacemos una predicción a partir del modelo piecewise

```{r}

# Inicializamos la variable, y la sustituimos donde no haya NA

datos$pred_piecewise_training <- NA  
datos$pred_piecewise_training[!is.na(datos$training_hours)] <- 
  predict(modelo_piecewise_training, newdata = datos[!is.na(datos$training_hours), ])


```

A continuación, hacemos la gráfica que permita ver la diferencia de esto frente a las observaciones reales.

```{r}
pred_group <- aggregate(pred_piecewise_training ~ training_group, datos, mean)

# Crear data frame para líneas horizontales
pred_lines <- data.frame(
  x = c(min(datos$training_hours_imputada), max(datos$training_hours_imputada),
        min(datos$training_hours_imputada), max(datos$training_hours_imputada)),
  y = c(pred_group$pred_piecewise_training[1], pred_group$pred_piecewise_training[1],
        pred_group$pred_piecewise_training[2], pred_group$pred_piecewise_training[2]),
  group_label = c("Grupo 1 (<20 h)", "Grupo 1 (<20 h)",
                  "Grupo 2 (>20 h)", "Grupo 2 (>20 h)")
)

# Gráfico
ggplot(datos, aes(x = training_hours_imputada)) +
  geom_point(aes(y = performance_index), alpha = 0.6, color = "steelblue") +
  geom_line(data = pred_lines, aes(x = x, y = y, color = group_label), linewidth = 1.2) +
  scale_color_manual(values = c("Grupo 1 (<20 h)" = "darkgreen", 
                                "Grupo 2 (>20 h)" = "firebrick")) +
  labs(title = "Predicciones del modelo piecewise vs. valores reales",
       x = "Training Hours",
       y = "Performance Index",
       color = "Grupo") +
  theme_minimal()





```

De esta manera, podemos ver las predicciones del modelo piecewise sobre las observaciones reales.

## 9. Ajuste de modelo lineal simple con casos completos

En primer lugar, vamos a ver cuántos casos perdemos por quedarnos solo con los completos.

```{r}

# Número total de filas
n_total <- nrow(datos)

# Casos completos
datos_completos <- datos[complete.cases(datos[, c("performance_index", "training_hours")]), ]

# Número de filas completas
n_completos <- nrow(datos_completos)

# Observaciones perdidas
n_perdidas <- n_total - n_completos

n_total
n_completos
n_perdidas



```

Vemos que perdemos 5 casos por valores faltantes.

A continuación, elaboramos el modelo´

```{r}

modelo_lineal_faltante <- lm(performance_index ~ training_hours, data = datos_completos)
summary(modelo_lineal_faltante)


```

## 10. Modelo con dataset imputado

Vamos a crear un dataset con todos los datos imputados

```{r}

# Crear copia del dataset original
datos_imputados <- datos

# Reemplazar variables con sus versiones imputadas
datos_imputados$training_hours <- training_hours_imputada
datos_imputados$fatigue <- fatigue_imputada_media
datos_imputados$sleep_hours <- sleep_imputada
datos_imputados$strength_1RM <- strength_1RM_imputada

# Imputar agility_time con la media
media_agility <- mean(datos$agility_time, na.rm = TRUE)
agility_time_imputada <- datos$agility_time
agility_time_imputada[is.na(agility_time_imputada)] <- media_agility
datos_imputados$agility_time <- agility_time_imputada


```

Ahora, planteamos el modelo con el nuevo dataset

```{r}

modelo_lineal_imputado <- lm(performance_index ~ training_hours, data = datos_imputados)
summary(modelo_lineal_imputado)

```

Cambia mínimamente el intercepto, pero es un cambio demasiado pequeño como para tenerlo en cuenta. Por otro lado, la pendiente sigue igual.

# 5. HETEROCEDASTICIDAD

## 1. Ajuste de modelo múltiple con gráfico de residuos

Vamos a crear el modelo múltiple con los datos imputados.

```{r}

modelo_multiple5.1 <- lm(
  performance_index ~ training_hours + match_intensity + strength_1RM + fatigue,
  data = datos_imputados
)

summary(modelo_multiple5.1)

```

A continuación, vamos a disponer el gráfico de residuos vs valores ajustados.

```{r}

ggplot(data = datos_imputados, aes(x = modelo_multiple5.1$fitted.values,
                                   y = modelo_multiple5.1$residuals)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_hline(yintercept = 0, color = "red", linewidth = 1) +
  labs(title = "Residuos vs Valores Ajustados (Modelo Múltiple)",
       x = "Valores ajustados",
       y = "Residuos") +
  theme_minimal()

```

En el gráfico podemos apreciar los residuos dispersos alrededor de 0 sin patrones aparentes, por lo que la relación lineal es razonable. Puede haber algo de heterocedasticidad, pero nada preocupante a priori.

## 2. Test de Breusch-Pagan

Aplicaremos al modelo recientemente creado el test mencionado para determinar si hay un problema de heterocedasticidad.

```{r}

bptest(modelo_multiple5.1)

```

El p-valor del test es muy elevado, por lo que no podemos rechazar la hipótesis de homocedasticidad. De esta manera, no podemos concluir que haya un problema de heterocedasticidad en el modelo.

## 3. Varianza de los residuos en la variable *fatigue*

Empezaremos extrayendo los residuos del modelo, dividiendo los casos en 3 niveles de fatiga (bajo, medio y alto) dividiendo la muestra en terciles para tomar grupos con el mismo número de muestras.

```{r}

# Residuos
residuos <- residuals(modelo_multiple5.1)

# Crear niveles bajo–medio–alto por terciles
datos_imputados$fatigue_grupo <- cut(
  datos_imputados$fatigue,
  breaks = quantile(datos_imputados$fatigue, probs = c(0, 1/3, 2/3, 1), na.rm = TRUE),
  labels = c("Bajo", "Medio", "Alto"),
  include.lowest = TRUE
)


```

A continuación, calculamos la varianza de los residuos en cada uno de los grupos.

```{r}

tapply(residuos, datos_imputados$fatigue_grupo, var)


```

Vemos que las varianzas son similares, pero con ligeras variaciones. Sin embargo, vamos a concluir si la diferencia entre estas es estadísticamente significativa. Para ello, usaremos el test de Bartlett.

```{r}

bartlett.test(residuos ~ datos_imputados$fatigue_grupo)

```

Como el p-valor es muy elevado, no podemos afirmar que la diferencia de varianzas entre los residuos sea estadísticamente significativa.

## 4. Gráfica de residuos vs *sleep_hours*

```{r}

# Obtener los residuos del modelo
residuos <- residuals(modelo_multiple5.1)
ajustados <- fitted(modelo_multiple5.1)


ggplot(datos_imputados, aes(x = sleep_hours, y = residuos)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_smooth(method = "loess", se = FALSE, color = "red", linewidth = 1) +
  labs(title = "Residuos vs. Horas de sueño",
       x = "Horas de sueño",
       y = "Residuos") +
  theme_minimal()


```

En este caso, vemos como los residuos se alejan del 0, por lo que podemos detectar un ligero problema de heterocedasticidad; ya que la varianza de los residuos parece incrementar conforme lo hacen las horas de sueño.

De esta manera, podemos intuir que la varianza depende de las horas de sueño.

## 5. Modelo con errores estándar robusto (HC)

```{r}

# 1. Errores estándar clásicos
clasico <- summary(modelo_multiple5.1)$coefficients[, 2]

# 2. Errores estándar robustos (HC3)
robustos <- sqrt(diag(vcovHC(modelo_multiple5.1, type = "HC3")))

# 3. Comparación en un data frame
comparacion <- data.frame(
  Coeficiente = names(clasico),
  SE_Clasico = clasico,
  SE_Robusto_HC3 = robustos,
  Aumento_relativo = robustos / clasico
)

comparacion

```

Vemos que los errores de los coeficientes cambian considerablemente con respecto de los del modelo original.

## 6. Transformación logarítmica al predictor *travel_km_week*

En primer lugar, creamos la variable transformada (le sumamos 1 para que no haya valores iguales a 0 y pueda tomar logaritmos).

```{r}

datos_imputados$log_travel_km_week <- log(datos_imputados$travel_km_week+1)

```

A continuación, vamos a hacer el modelo con la variable ajustada

```{r}

modelo_multiple5.6 <- lm(
  performance_index ~ training_hours + match_intensity + strength_1RM + fatigue + log_travel_km_week,
  data = datos_imputados
)

# Resumen del modelo
summary(modelo_multiple5.6)

```

Para ver si mejora la heterocedasticidad dibujaremos la gráfica de residuos vs valores ajustados de este nuevo modelo

```{r}

# Para el modelo original
plot(modelo_multiple5.1$fitted.values, modelo_multiple5.1$residuals,
     xlab = "Valores ajustados", ylab = "Residuos",
     main = "Residuos vs valores ajustados (modelo original)")
abline(h = 0, col = "red")

# Para el modelo con log
plot(modelo_multiple5.6$fitted.values, modelo_multiple5.6$residuals,
     xlab = "Valores ajustados", ylab = "Residuos",
     main = "Residuos vs valores ajustados (modelo log)")
abline(h = 0, col = "red")


```

La gráfica es similar, pero podemos apreciar que los valores tienden a concetrarse en mayor medida en el centro de la gráfica. De esta manera, podemos intuir que se "corrige" un poco el ligero problema de heterocedasticidad que podíamos haber tenido antes.

De todas formas, vamos a verlo con el test Breusch-Pagan

```{r}

# Modelo original
bptest(modelo_multiple5.1)

# Modelo log-transformado
bptest(modelo_multiple5.6)

```

Como ambos p-valores son mucho mayores que 0.05, concluimos que no hay diferencia significativa a pesar del ligero aporte que se haya podido dar al modelo.

## 7. Estudio gráfico sobre la causa de la heterocedasticidad

Tomamos en primer lugar los residuos estandarizados.

```{r}

residuos5.6 <- rstandard(modelo_multiple5.6)

```

Ahora, vamos a ver si la heterocedasticidad depende de la fatiga

```{r}

plot(datos_imputados$fatigue, residuos,
     xlab = "Fatiga",
     ylab = "Residuos estandarizados",
     main = "Residuos vs Fatiga")
abline(h = 0, col = "red")

```

Los valores aparecen de manera uniforme sin patrones claros, por lo que los residuos de los jugadores con altos valores de fatiga no parecen sufrir de heterocedasticidad.

Por otro lado, vamos a ver si esta depende de las horas de sueño

```{r}

plot(datos_imputados$sleep_hours, residuos,
     xlab = "Horas de sueño",
     ylab = "Residuos estandarizados",
     main = "Residuos vs Sueño")
abline(h = 0, col = "red")

```

Los residuos de jugadores con pocas horas de sueño aparecen más dispersos que los valores de la anterior gráfica.

Por tanto, podemos concluir que el efecto de la heterocedasticidad es más acusado para jugadores con pocas horas de sueño que para aquellos con mucha fatiga.

## 8. Cómo afecta la heterocedasticidad

La heterocedasticidad es, por definición, el fenómeno ocurrido cuando la varianza de los errores cambia según los valores de los predictores.

En una primera instancia, esto implica que los estimadores de los coeficientes no sean de mínima varianza, perdiendo así eficacia.

De esta manera, la varianza obtenida a partir de los estimadores a la hora de hacer intervalos de confianza, se estimará incorrectamente; derivando en intervalos de confianza que no reflejen correctamente el umbral real.

Con respecto a los tests de significación, tenemos un problema similar. En este caso, lo que cambiará será el p-valor de los mismos. En particular, en caso de sobreestimar varianza, tendremos riesgos de encontrar falsos negativos (es decir, de no encontrar un efecto real) , y en caso de subestimar varianza estará el riesgo de encontrar falsos positivos (es decir, rechazar la hipótesis nula siendo cierta).

## 9. Posibilidad de recurrir a un GLM con varianza distinta a la normal para modelar *performance_index*

Sí, en algunos casos, recurrir a un GLM con varianza distinta a la normal puede ser conveniente. Esto es debido a que, usando un GLM adecuado, podemos separar la media de la varianza, permitiendo mejorar la eficiencia de los intervalos de confianza y tests.

Es decir, el uso de los GLM puede ayudar en caso de problemas con heterocedasticidad marcada, valores extremos o asimetría y restricciones de la variable.

## 10. Estrategia más adecuada

En este caso, lo mejor será dejar el modelo tal cual está, ya que no tenemos grandes problemas de heterocedasticidad que puedan dar lugar a errores posteriores con tests o intervalos de confianza.

De esta manera, al no haber heterocedasticidad marcada ni una gran diferencia entre residuos normales y robustos, lo mejor será mantener la regresión múltiple estándar.

# 6. MODELOS GLM

## 1. Modelo de Poisson para *injury_count*

Vamos a ajustar un modelo de Poisson para explicar la variable *injury_count* a partir de *training_hours* y *fatigue*.

```{r}

modelo_poisson <- glm(injuries_count ~ training_hours + fatigue,poisson(link="log"), data = datos, )

summary(modelo_poisson)

```

Con la salida del modelo, vamos a ver si los coeficientes son significativos. Como el p-valor del intercepto es muy bajo (<0.05), significa que es significativo. Por el contrario, los otros coeficientes no lo serán, ya que su p-valor asociado es >0.05.

## 2. Existencia de sobre-dispersión en el modelo de Poisson

El índice de dispersión viene determinado por el cociente entre la desviación residual y los grados de libertad de los residuos.

```{r}

modelo_poisson$deviance / modelo_poisson$df.residual

```

En este caso, el índice de dispersión es 0.9772406. De esta manera, al aproximarse a 1, la dispersión que presenta el modelo es adecuada.

## 3. Modelo de binomial negativa

```{r}

modelo_nb <- glm.nb(injuries_count ~ training_hours + fatigue, data = datos)
summary(modelo_nb)

```

Para ver cual es mejor, comapararemos el AIC de los modelos

```{r}

AIC(modelo_poisson, modelo_nb)

```

Ambos valores son similares, por lo que los modelos también lo serán. Sin embargo, el AIC del modelo binomial negativo es algo menor, por lo que el modelo será algo mejor que el modelo de Poisson.

## 4. Comparación de la devianza residual de los modelos de Poisson y NegBin

```{r}

cat("Poisson:    deviance =", deviance(modelo_poisson),
    "  df.resid =", modelo_poisson$df.residual, "\n")
cat("NegBin:     deviance =", deviance(modelo_nb),
    "  df.resid =", modelo_nb$df.residual, "\n")

# razón simple (devianza/df) para cada modelo
cat("Poisson: dev/df =", deviance(modelo_poisson) / modelo_poisson$df.residual, "\n")
cat("NegBin:  dev/df =", deviance(modelo_nb) / modelo_nb$df.residual, "\n")

# diferencia de devianzas y test de razón de verosimilitud (lrtest)
cat("Diferencia de devianzas (Poisson - NegBin) =",
    deviance(modelo_poisson) - deviance(modelo_nb), "\n")

# lrtest
lr <- lrtest(modelo_poisson, modelo_nb)
print(lr)

```










